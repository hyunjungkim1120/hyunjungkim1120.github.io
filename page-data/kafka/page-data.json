{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/kafka",
    "result": {"data":{"markdownRemark":{"id":"d98a8f95-11c9-5886-abd4-e62a8b18bc7b","html":"<h2>Apache Kafka</h2>\n<p>카프카를 검색하면 나오는 설명은 아래와 같다.</p>\n<p> <em>아파치 카프카는 아파치 소프트웨어 재단이 스칼라로 개발한 오픈 소스 메시지 브로커 프로젝트이다.</em>\n<em>이 프로젝트는 실시간 데이터 피드를 관리하기 위해 통일된, 높은 처리량, 낮은 지연시간을 지닌 플랫폼을 제공하는 것이 목표이다.</em></p>\n<p><strong>쉽게 말해서 카프카는 분산환경에 특화되어있는 \"메시지 큐\" 이다.</strong></p>\n</br>\n</br>\n</br>\n<p>Kafka와 함께 자주 언급되는 메시징 시스템은 RabbitMQ, ActiveMQ 등이 있다. 잠깐 Kafka를 다른 메시징 시스템과 비교해보고 넘어가자.</p>\n<ul>\n<li>Kafka는 Message Broker가 Consumer에게 메시지를 push하는 방식의 RabbitMQ와는 달리 Consumer가 Broker로 부터 메시지를 Pull하는 방식이다. Consumer가 메시지를 Pull하는 카프카의 방식은 Consumer에게 부하를 상대적으로 덜 주며 Consumer는 자신이 처리할 수 있을 때 메시지를 가져오므로 자원을 효율적으로 사용한다.</li>\n<li>많은 사람들이 공통적으로 말하는 kafka 의 장점은 크게 <strong>1.데이터의 영속성 보장 2. 메시지 유실위험이 적고 에러복구용이, 실시간 로그처리에 특화</strong> 라고 이야기한다. </li>\n</ul>\n</br>\n</br>\n<p>카프카의 구성요소로는 크게 <strong>Producer, Consumer, Event , Topic</strong> 으로 구분할 수 있다.</p>\n</br>\n</br>\n<p><strong>Producer</strong>  : 원하는 <strong>Topic</strong>으로 메시지를 보낸다. </p>\n<p><strong>Consumer</strong> : 레코드/메시지를 처리하는 엔터티이다. 여러개의 Consumer가 그룹 묶어서 구성되어있으며 하나의 Consumer 그룹은 하나의 Topic에만 접근하여 메세지를 Pull 한다.</p>\n<ul>\n<li>여기서 Topic 기준으로 생각하면 여러개의 Consumer 그룹이 하나의 토픽을 구독할 수 있다!</li>\n</ul>\n<p><strong>Topic</strong> : Apache Kafka 및 기타 메시징 솔루션에서 토픽은 메시지에 대한 관심을 표시하는 데 사용되는 주소 지정이 가능한 추상화 계층이다. 토픽은 게시(Publish) 및 구독(Subscribe)할 수 있다.</p>\n<p>다시한번 쉽게 풀어설명하자면 Producer는 메세지를 보내고 Consumer는 메세지를 꺼내온다!</p>\n</br>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 861px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 41.79687500000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/svg+xml,%3csvg%20xmlns=\\'http://www.w3.org/2000/svg\\'%20width=\\'400\\'%20height=\\'168\\'%20viewBox=\\'0%200%20400%20168\\'%20preserveAspectRatio=\\'none\\'/%3e'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"kafka_example\"\n        title=\"kafka_example\"\n        src=\"/static/37c597f34f7ec6615ffc1a7684e56edd/e35ec/kafka_example.png\"\n        srcset=\"/static/37c597f34f7ec6615ffc1a7684e56edd/6f3f2/kafka_example.png 256w,\n/static/37c597f34f7ec6615ffc1a7684e56edd/01e7c/kafka_example.png 512w,\n/static/37c597f34f7ec6615ffc1a7684e56edd/e35ec/kafka_example.png 861w\"\n        sizes=\"(max-width: 861px) 100vw, 861px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">kafka_example</figcaption>\n  </figure></p>\n</br>\n<p><a href=\"https://github.com/confluentinc/examples/tree/6.2.0-post/clients/cloud\">https://github.com/confluentinc/examples/tree/6.2.0-post/clients/cloud</a></p>\n<p>해당 주소에 각 언어에 맞는 샘플 코드를 참고하여 각자의 상황에 맞는 코드를 작성할 수 있다.</p>\n</br>\n</br>\n<p>아래는 C#으로 카프카 Cunsumer를 작성한 예시이다. Confluent.Kafka 1.7.0을 Nuget에서 다운받았다. 그리고 따로 ConsumerConfig 파일을 작성하여 설정 값들을 작성하였다. 만약 따로 설정 파일을 만들지 않는다면 region ConsumerConfig 가져오기 ~endregion 부분에 각각 해당 변수에 값을 할당하면 된다.</p>\n</br>\n<div class=\"gatsby-highlight\" data-language=\"c#\"><pre class=\"language-c#\"><code class=\"language-c#\">public Task GetKafkaMessage()\n        {\n            #region ConsumerConfig 가져오기\n            var config = new ConsumerConfig\n            {\n                GroupId = _configuration.GetValue&lt;string&gt;(&quot;KafkaConfig:GroupId&quot;),\n                BootstrapServers = _configuration.GetValue&lt;string&gt;(&quot;KafkaConfig:BootstrapServers&quot;),\n                SaslUsername = _configuration.GetValue&lt;string&gt;(&quot;KafkaConfig:SaslUsername&quot;),\n                SaslPassword = _configuration.GetValue&lt;string&gt;(&quot;KafkaConfig:SaslPassword&quot;),\n                SecurityProtocol = (SecurityProtocol)Enum.Parse(typeof(SecurityProtocol), _configuration.GetValue&lt;string&gt;(&quot;KafkaConfig:SecurityProtocol&quot;)),\n                SaslMechanism = (SaslMechanism)Enum.Parse(typeof(SaslMechanism), _configuration.GetValue&lt;string&gt;(&quot;KafkaConfig:SaslMechanism&quot;)),\n                AutoOffsetReset = (AutoOffsetReset)Enum.Parse(typeof(AutoOffsetReset), _configuration.GetValue&lt;string&gt;(&quot;KafkaConfig:AutoOffsetReset&quot;)),\n\n            };\n            #endregion ConsumerConfig 가져오기\n\n            using (var consumer = new ConsumerBuilder&lt;string, string&gt;(config)\n                .SetErrorHandler((_, e) =&gt; _logger.LogWarning($&quot;Error: {e.Reason}&quot;))\n                .Build())\n            {\n                string topic = _configuration.GetValue&lt;string&gt;(&quot;KafkaConfig:topic&quot;);\n                consumer.Subscribe(topic);\n\n                List&lt;string&gt; InsertDataList = new List&lt;string&gt;();\n                String message = &quot;&quot;;\n                DateTime LastUpdateDate = DateTime.Now;\n\n                try\n                {\n                    while (true)\n                    {\n                        try\n                        {\n                            #region 일정시간, listQty 초과시 로그\n                            TimeSpan diffDate = DateTime.Now - LastUpdateDate;\n\n                            //마지막 update시간이 3초 지났거나 데이터가 10건이상이면\n                            if (InsertDataList.Count&gt;0 &amp;&amp; (diffDate.Seconds &gt; 3 || InsertDataList.Count &gt;= 10))\n                            {\n                                foreach (var data in InsertDataList)\n                                {\n\t\t\t\t\t\t\t\t\t_logger.LogInformation(data);\n                                }\n                                InsertDataList.Clear();\n                            }\n                            #endregion 일정시간, listQty 초과시 로그\n\n                            var consumeResult = consumer.Consume(5000);\n                           \n                            //broker에서 가져온 data가 있는 경우\n                            if(consumeResult != null)\n                            {\n                                message = consumeResult.Value;\n                                InsertDataList.Add(message);\n                            }\n                        }\n                        catch (ConsumeException e)\n                        {\n                            _logger.LogWarning($&quot;Error occurred: {e.Error.Reason}&quot;);\n                        }\n                    }\n                }\n                catch (Exception e)\n                {\n                    _logger.LogWarning($&quot;Exception occurred: {e.Message}&quot;);\n                    consumer.Close();\n                }\n            }\n            return Task.CompletedTask;\n        }</code></pre></div>\n</br>\n</br>\n</br>\n<p>Producer로 메시지를 보내는 부분은 Serilog로 로그를 전송하였다.</p>\n<ul>\n<li>Serilog >v2.9.0</li>\n<li>Serilog.Sinks.PeriodicBatching >v2.3.0</li>\n<li>Confluent.Kafka >v1.6.3</li>\n</ul>\n<p>위의 조건을 준수하여 패키지를 받고 아래처럼 설정 후 프로그램 실행 로그를 사용하였다.</p>\n<div class=\"gatsby-highlight\" data-language=\"c#\"><pre class=\"language-c#\"><code class=\"language-c#\">try\n{\n\t//kafka Log 전송 설정\n\tkafkaLogger = new LoggerConfiguration()\n\t\t\t\t.WriteTo.Kafka(\n\t\t\t\t\t\tbatchSizeLimit: 10, period: 2,\n                        topic: &quot;log-events&quot;,\n                        bootstrapServers: &quot;서버 주소&quot;, \n                        saslMechanism: SaslMechanism.Plain,\n                        securityProtocol: SecurityProtocol.SaslPlaintext,\n                        saslUsername: &quot;$ConnectionString&quot;,\n                        saslPassword: &quot;my-event-hub-instance-connection-string&quot;\n                    .CreateLogger();\n} catch (Exception ex)\n{\n\t_logger.Warning(&quot;kafka connection error, Message : {0}&quot;, ex.Message);\n}</code></pre></div>\n</br>\n<h4>Parameters</h4>\n<ul>\n<li><strong>bootstrapServers</strong> - Comma separated list of Kafka Bootstrap Servers. Defaults to \"localhost:9092\"</li>\n<li><strong>batchSizeLimit</strong> - Maximum number of logs to batch. Defaults to 50</li>\n<li><strong>period</strong> - The period in seconds to send batches of logs. Defaults to 5 seconds</li>\n<li><strong>securityProtocol</strong> - SecurityProtocol.Plaintext</li>\n<li><strong>saslMechanism</strong> - The SASL Mecahnism. Defaults to SaslMechanism.Plain</li>\n<li><strong>topic</strong> - Name of the Kafka topic. Defaults to \"logs\"</li>\n<li><strong>topicDecider</strong> - Alternative to a static/constant \"topic\" value. Function that can be used to determine the topic to be written to at runtime (example below)</li>\n<li><strong>saslUsername</strong> - (Optional) Username for SASL. This is required for Azure Event Hubs and should be set to <code class=\"language-text\">$ConnectionString</code></li>\n<li><strong>saslPassword</strong> - (Optional) Password for SASL. This is required for Azure Event Hubs and is your entire Connection String.</li>\n<li><strong>sslCaLocation</strong> - (Optional) Location of the SSL CA Certificates This is required for Azure Event Hubs and should be set to <code class=\"language-text\">./cacert.pem</code> as this package includes the Azure carcert.pem file which is copied into your binary output directory.</li>\n<li><strong>formatter</strong> - optional <code class=\"language-text\">ITextFormatter</code> you can specify to format log entries. Defaults to the standard <code class=\"language-text\">JsonFormatter</code> with <code class=\"language-text\">renderMessage</code> set to <code class=\"language-text\">true</code>.</li>\n</ul>\n</br>\n</br>\n</br>\n<p>사실 더 깊게 알고자 하면 Kafka는 Commit Offset을 어떻게 제어하는가로 전달보증을 확실히 제어하기도 하는 등 </p>\n<p>자세히 알고 쓸수록 대규모 데이터 처리나 응답속도가 중요한 서비스에서 유용하게 사용할 수 있다!</p>\n</br>\n<p>하단의 주소는 Kafka를 이해하는데 있어서 유용한 링크이다.</p>\n<p><a href=\"https://t3guild.com/2020/04/18/kafka-%ea%b0%9c%ec%9a%94/\">https://t3guild.com/2020/04/18/kafka-%ea%b0%9c%ec%9a%94/</a></p>\n</br>","excerpt":"Apache Kafka…","frontmatter":{"date":"September 09, 2021","slug":"/kafka","title":"Kafka","description":"Apache Kafka 개념이해 및 사용 (c#)","featuredImage":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/cbef0d6811fcdc13018eecc38f8d1257/e6ff9/kafka.jpg","srcSet":"/static/cbef0d6811fcdc13018eecc38f8d1257/06a47/kafka.jpg 750w,\n/static/cbef0d6811fcdc13018eecc38f8d1257/3c27a/kafka.jpg 1080w,\n/static/cbef0d6811fcdc13018eecc38f8d1257/28136/kafka.jpg 1366w,\n/static/cbef0d6811fcdc13018eecc38f8d1257/e6ff9/kafka.jpg 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/cbef0d6811fcdc13018eecc38f8d1257/45f0d/kafka.webp 750w,\n/static/cbef0d6811fcdc13018eecc38f8d1257/eedfa/kafka.webp 1080w,\n/static/cbef0d6811fcdc13018eecc38f8d1257/e048d/kafka.webp 1366w,\n/static/cbef0d6811fcdc13018eecc38f8d1257/f594a/kafka.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5593750000000001}}}}}},"pageContext":{"id":"d98a8f95-11c9-5886-abd4-e62a8b18bc7b","previous":{"id":"3f8c7e3c-c266-588e-a368-e2f34ecc4dd0","frontmatter":{"slug":"/redis","template":"blog-post","title":"redis (1)"}},"next":{"id":"6eae2c69-0ee8-5d4c-ad4d-0d12523028ea","frontmatter":{"slug":"/행렬테두리회전하기","template":"algorithm-post","title":"행렬 테두리 회전하기(프로그래머스) Java"}}}},
    "staticQueryHashes": ["228695001","2744905544","358227665"]}