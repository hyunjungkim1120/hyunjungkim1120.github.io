---
title: Kafka
template: blog-post
tags: [ Kafka ]
date: 2021-09-09T05:25:44.226Z
slug: /kafka
featuredImage: /assets/Kafka/kafka.png
description: Apache Kafka 개념이해 및 사용 (c#)

---



## Apache Kafka

카프카를 검색하면 나오는 설명은 아래와 같다.

 *아파치 카프카는 아파치 소프트웨어 재단이 스칼라로 개발한 오픈 소스 메시지 브로커 프로젝트이다.*
*이 프로젝트는 실시간 데이터 피드를 관리하기 위해 통일된, 높은 처리량, 낮은 지연시간을 지닌 플랫폼을 제공하는 것이 목표이다.*

**쉽게 말해서 카프카는 분산환경에 특화되어있는 "메시지 큐" 이다.**

</br>

</br>

</br>

Kafka와 함께 자주 언급되는 메시징 시스템은 RabbitMQ, ActiveMQ 등이 있다. 잠깐 Kafka를 다른 메시징 시스템과 비교해보고 넘어가자.

* Kafka는 Message Broker가 Consumer에게 메시지를 push하는 방식의 RabbitMQ와는 달리 Consumer가 Broker로 부터 메시지를 Pull하는 방식이다. Consumer가 메시지를 Pull하는 카프카의 방식은 Consumer에게 부하를 상대적으로 덜 주며 Consumer는 자신이 처리할 수 있을 때 메시지를 가져오므로 자원을 효율적으로 사용한다.
* 많은 사람들이 공통적으로 말하는 kafka 의 장점은 크게 **1.데이터의 영속성 보장 2. 메시지 유실위험이 적고 에러복구용이, 실시간 로그처리에 특화** 라고 이야기한다. 

</br>

</br>

카프카의 구성요소로는 크게 **Producer, Consumer, Event , Topic** 으로 구분할 수 있다.

</br>

</br>

**Producer**  : 원하는 **Topic**으로 메시지를 보낸다. 

**Consumer** : 레코드/메시지를 처리하는 엔터티이다. 여러개의 Consumer가 그룹 묶어서 구성되어있으며 하나의 Consumer 그룹은 하나의 Topic에만 접근하여 메세지를 Pull 한다.

- 여기서 Topic 기준으로 생각하면 여러개의 Consumer 그룹이 하나의 토픽을 구독할 수 있다!

**Topic** : Apache Kafka 및 기타 메시징 솔루션에서 토픽은 메시지에 대한 관심을 표시하는 데 사용되는 주소 지정이 가능한 추상화 계층이다. 토픽은 게시(Publish) 및 구독(Subscribe)할 수 있다.

다시한번 쉽게 풀어설명하자면 Producer는 메세지를 보내고 Consumer는 메세지를 꺼내온다!

</br>

![kafka_example](/assets/Kafka/kafka_example.png)





</br>

https://github.com/confluentinc/examples/tree/6.2.0-post/clients/cloud

해당 주소에 각 언어에 맞는 샘플 코드를 참고하여 각자의 상황에 맞는 코드를 작성할 수 있다.

</br>

</br>

아래는 C#으로 카프카 Cunsumer를 작성한 예시이다. Confluent.Kafka 1.7.0을 Nuget에서 다운받았다. 그리고 따로 ConsumerConfig 파일을 작성하여 설정 값들을 작성하였다. 만약 따로 설정 파일을 만들지 않는다면 region ConsumerConfig 가져오기 ~endregion 부분에 각각 해당 변수에 값을 할당하면 된다.

</br>

```c#
public Task GetKafkaMessage()
        {
            #region ConsumerConfig 가져오기
            var config = new ConsumerConfig
            {
                GroupId = _configuration.GetValue<string>("KafkaConfig:GroupId"),
                BootstrapServers = _configuration.GetValue<string>("KafkaConfig:BootstrapServers"),
                SaslUsername = _configuration.GetValue<string>("KafkaConfig:SaslUsername"),
                SaslPassword = _configuration.GetValue<string>("KafkaConfig:SaslPassword"),
                SecurityProtocol = (SecurityProtocol)Enum.Parse(typeof(SecurityProtocol), _configuration.GetValue<string>("KafkaConfig:SecurityProtocol")),
                SaslMechanism = (SaslMechanism)Enum.Parse(typeof(SaslMechanism), _configuration.GetValue<string>("KafkaConfig:SaslMechanism")),
                AutoOffsetReset = (AutoOffsetReset)Enum.Parse(typeof(AutoOffsetReset), _configuration.GetValue<string>("KafkaConfig:AutoOffsetReset")),

            };
            #endregion ConsumerConfig 가져오기

            using (var consumer = new ConsumerBuilder<string, string>(config)
                .SetErrorHandler((_, e) => _logger.LogWarning($"Error: {e.Reason}"))
                .Build())
            {
                string topic = _configuration.GetValue<string>("KafkaConfig:topic");
                consumer.Subscribe(topic);

                List<string> InsertDataList = new List<string>();
                String message = "";
                DateTime LastUpdateDate = DateTime.Now;

                try
                {
                    while (true)
                    {
                        try
                        {
                            #region 일정시간, listQty 초과시 로그
                            TimeSpan diffDate = DateTime.Now - LastUpdateDate;

                            //마지막 update시간이 3초 지났거나 데이터가 10건이상이면
                            if (InsertDataList.Count>0 && (diffDate.Seconds > 3 || InsertDataList.Count >= 10))
                            {
                                foreach (var data in InsertDataList)
                                {
									_logger.LogInformation(data);
                                }
                                InsertDataList.Clear();
                            }
                            #endregion 일정시간, listQty 초과시 로그

                            var consumeResult = consumer.Consume(5000);
                           
                            //broker에서 가져온 data가 있는 경우
                            if(consumeResult != null)
                            {
                                message = consumeResult.Value;
                                InsertDataList.Add(message);
                            }
                        }
                        catch (ConsumeException e)
                        {
                            _logger.LogWarning($"Error occurred: {e.Error.Reason}");
                        }
                    }
                }
                catch (Exception e)
                {
                    _logger.LogWarning($"Exception occurred: {e.Message}");
                    consumer.Close();
                }
            }
            return Task.CompletedTask;
        }
```

</br>

</br>

</br>

Producer로 메시지를 보내는 부분은 Serilog로 로그를 전송하였다.

- Serilog >v2.9.0
- Serilog.Sinks.PeriodicBatching >v2.3.0
- Confluent.Kafka >v1.6.3

위의 조건을 준수하여 패키지를 받고 아래처럼 설정 후 프로그램 실행 로그를 사용하였다.

```c#
try
{
	//kafka Log 전송 설정
	kafkaLogger = new LoggerConfiguration()
				.WriteTo.Kafka(
						batchSizeLimit: 10, period: 2,
                        topic: "log-events",
                        bootstrapServers: "서버 주소", 
                        saslMechanism: SaslMechanism.Plain,
                        securityProtocol: SecurityProtocol.SaslPlaintext,
                        saslUsername: "$ConnectionString",
                        saslPassword: "my-event-hub-instance-connection-string"
                    .CreateLogger();
} catch (Exception ex)
{
	_logger.Warning("kafka connection error, Message : {0}", ex.Message);
}
```

</br>

#### Parameters

- **bootstrapServers** - Comma separated list of Kafka Bootstrap Servers. Defaults to "localhost:9092"
- **batchSizeLimit** - Maximum number of logs to batch. Defaults to 50
- **period** - The period in seconds to send batches of logs. Defaults to 5 seconds
- **securityProtocol** - SecurityProtocol.Plaintext
- **saslMechanism** - The SASL Mecahnism. Defaults to SaslMechanism.Plain
- **topic** - Name of the Kafka topic. Defaults to "logs"
- **topicDecider** - Alternative to a static/constant "topic" value. Function that can be used to determine the topic to be written to at runtime (example below)
- **saslUsername** - (Optional) Username for SASL. This is required for Azure Event Hubs and should be set to `$ConnectionString`
- **saslPassword** - (Optional) Password for SASL. This is required for Azure Event Hubs and is your entire Connection String.
- **sslCaLocation** - (Optional) Location of the SSL CA Certificates This is required for Azure Event Hubs and should be set to `./cacert.pem` as this package includes the Azure carcert.pem file which is copied into your binary output directory.
- **formatter** - optional `ITextFormatter` you can specify to format log entries. Defaults to the standard `JsonFormatter` with `renderMessage` set to `true`.

</br>

</br>

</br>

사실 더 깊게 알고자 하면 Kafka는 Commit Offset을 어떻게 제어하는가로 전달보증을 확실히 제어하기도 하는 등 

자세히 알고 쓸수록 대규모 데이터 처리나 응답속도가 중요한 서비스에서 유용하게 사용할 수 있다!

</br>

하단의 주소는 Kafka를 이해하는데 있어서 유용한 링크이다.

https://t3guild.com/2020/04/18/kafka-%ea%b0%9c%ec%9a%94/



</br>
